{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "005b0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd2344",
   "metadata": {},
   "source": [
    "# Векторизация\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a74ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_frame = pd.read_csv('bib_data_union_v3.csv.zip', compression='zip')\n",
    "# text_data_frame = pd.read_csv('text_small_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "273968f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_record</th>\n",
       "      <th>style_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ num ] sp upword , sp caplet . sp caplet . sp...</td>\n",
       "      <td>iaea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ capword sp othword sp capword ( year ) ] sp ...</td>\n",
       "      <td>bestpapers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ capword ( year ) ] sp caplet . sp caplet . s...</td>\n",
       "      <td>bestpapers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ capword sp othword sp capword ( year ) ] sp ...</td>\n",
       "      <td>bestpapers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ capword ( year ) ] sp capword sp caplet . sp...</td>\n",
       "      <td>bestpapers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    tokenized_record  style_name\n",
       "0  [ num ] sp upword , sp caplet . sp caplet . sp...        iaea\n",
       "1  [ capword sp othword sp capword ( year ) ] sp ...  bestpapers\n",
       "2  [ capword ( year ) ] sp caplet . sp caplet . s...  bestpapers\n",
       "3  [ capword sp othword sp capword ( year ) ] sp ...  bestpapers\n",
       "4  [ capword ( year ) ] sp capword sp caplet . sp...  bestpapers"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84742ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data_frame = text_data_frame.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f237a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alrabosh/miniconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "corpus = text_data_frame.tokenized_record\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda txt: txt.split(), \n",
    "                             ngram_range=(1, 2), \n",
    "                             token_pattern=r'\\S*')\n",
    "vectorized_data = vectorizer.fit_transform(corpus).toarray()\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a4fde5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=vectorized_data, columns=vectorizer.get_feature_names_out())\n",
    "df['style_name'] = text_data_frame.style_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c6478a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# big_df, small_df = train_test_split(df, test_size=0.017, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e0ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15026a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6007454"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a5be3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\"</th>\n",
       "      <th>\" (</th>\n",
       "      <th>\" )</th>\n",
       "      <th>\" ,</th>\n",
       "      <th>\" .</th>\n",
       "      <th>\" :</th>\n",
       "      <th>\" ;</th>\n",
       "      <th>\" ]</th>\n",
       "      <th>\" caplet</th>\n",
       "      <th>\" capword</th>\n",
       "      <th>...</th>\n",
       "      <th>— caplet</th>\n",
       "      <th>— capword</th>\n",
       "      <th>— num</th>\n",
       "      <th>— othword</th>\n",
       "      <th>— smallet</th>\n",
       "      <th>— sp</th>\n",
       "      <th>— upword</th>\n",
       "      <th>— year</th>\n",
       "      <th>— —</th>\n",
       "      <th>style_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>iaea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bestpapers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bestpapers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bestpapers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bestpapers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 486 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     \"  \" (  \" )  \" ,  \" .  \" :  \" ;  \" ]  \" caplet  \" capword  ...  — caplet  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0       0.0        0.0  ...       0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0       0.0        0.0  ...       0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0       0.0        0.0  ...       0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0       0.0        0.0  ...       0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0       0.0        0.0  ...       0.0   \n",
       "\n",
       "   — capword  — num  — othword  — smallet  — sp  — upword  — year  — —  \\\n",
       "0        0.0    0.0        0.0        0.0   0.0       0.0     0.0  0.0   \n",
       "1        0.0    0.0        0.0        0.0   0.0       0.0     0.0  0.0   \n",
       "2        0.0    0.0        0.0        0.0   0.0       0.0     0.0  0.0   \n",
       "3        0.0    0.0        0.0        0.0   0.0       0.0     0.0  0.0   \n",
       "4        0.0    0.0        0.0        0.0   0.0       0.0     0.0  0.0   \n",
       "\n",
       "   style_name  \n",
       "0        iaea  \n",
       "1  bestpapers  \n",
       "2  bestpapers  \n",
       "3  bestpapers  \n",
       "4  bestpapers  \n",
       "\n",
       "[5 rows x 486 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2232edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def encode(df):\n",
    "    columnsToEncode = list(df[['style_name']])\n",
    "    le = LabelEncoder()\n",
    "    for feature in columnsToEncode:\n",
    "        try:\n",
    "            df[feature] = le.fit_transform(df[feature])\n",
    "        except:\n",
    "            print('Error encoding '+feature)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e10f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = encode(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f5f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIQUE_TOKENS_N = len(list(df)) - 1\n",
    "UNIQUE_LABELS_N = len(set(df.style_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43cf4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=0)\n",
    "train_X, train_y, test_X, test_y = train_df.drop(['style_name'], axis=1), train_df.style_name, \\\n",
    "                                   test_df.drop(['style_name'], axis=1), test_df.style_name\n",
    "train_X, test_X = csr_matrix(train_X.values), csr_matrix(test_X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6995af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9deab75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test_y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5867b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# заимствованный код из библиотеки dlnlputils\n",
    "class SparseFeaturesDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n",
    "        cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n",
    "        return cur_features, cur_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d26d264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import random\n",
    "import traceback\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# заимствованный код из библиотеки dlnlputils\n",
    "def copy_data_to_device(data, device):\n",
    "    if torch.is_tensor(data):\n",
    "        return data.to(device)\n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        return [copy_data_to_device(elem, device) for elem in data]\n",
    "    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n",
    " \n",
    "def train_eval_loop(model, train_dataset, val_dataset, criterion,\n",
    "                    lr=1e-4, epoch_n=10, batch_size=32,\n",
    "                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n",
    "                    max_batches_per_epoch_train=10000,\n",
    "                    max_batches_per_epoch_val=1000,\n",
    "                    data_loader_ctor=DataLoader,\n",
    "                    optimizer_ctor=None,\n",
    "                    lr_scheduler_ctor=None,\n",
    "                    shuffle_train=True,\n",
    "                    dataloader_workers_n=0):\n",
    "    \"\"\"\n",
    "    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n",
    "    :param model: torch.nn.Module - обучаемая модель\n",
    "    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n",
    "    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n",
    "    :param criterion: функция потерь для настройки модели\n",
    "    :param lr: скорость обучения\n",
    "    :param epoch_n: максимальное количество эпох\n",
    "    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n",
    "    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n",
    "    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n",
    "        отсутствие улучшения модели, чтобы обучение продолжалось.\n",
    "    :param l2_reg_alpha: коэффициент L2-регуляризации\n",
    "    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n",
    "    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n",
    "    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n",
    "        (по умолчанию torch.utils.data.DataLoader)\n",
    "    :return: кортеж из двух элементов:\n",
    "        - среднее значение функции потерь на валидации на лучшей эпохе\n",
    "        - лучшая модель\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "\n",
    "    if optimizer_ctor is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n",
    "    else:\n",
    "        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n",
    "\n",
    "    if lr_scheduler_ctor is not None:\n",
    "        lr_scheduler = lr_scheduler_ctor(optimizer)\n",
    "    else:\n",
    "        lr_scheduler = None\n",
    "\n",
    "    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n",
    "                                        num_workers=dataloader_workers_n)\n",
    "    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                      num_workers=dataloader_workers_n)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch_i = 0\n",
    "    best_model = copy.deepcopy(model)\n",
    "\n",
    "    for epoch_i in range(epoch_n):\n",
    "        try:\n",
    "            epoch_start = datetime.datetime.now()\n",
    "            print('Эпоха {}'.format(epoch_i))\n",
    "\n",
    "            model.train()\n",
    "            mean_train_loss = 0\n",
    "            train_batches_n = 0\n",
    "            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
    "                if batch_i > max_batches_per_epoch_train:\n",
    "                    break\n",
    "\n",
    "                batch_x = copy_data_to_device(batch_x, device)\n",
    "                batch_y = copy_data_to_device(batch_y, device)\n",
    "\n",
    "                pred = model(batch_x)\n",
    "                loss = criterion(pred, batch_y)\n",
    "\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                mean_train_loss += float(loss)\n",
    "                train_batches_n += 1\n",
    "\n",
    "            mean_train_loss /= train_batches_n\n",
    "            print('Эпоха: {} итераций, {:0.2f} сек'.format(train_batches_n,\n",
    "                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\n",
    "            print('Среднее значение функции потерь на обучении', mean_train_loss)\n",
    "\n",
    "\n",
    "\n",
    "            model.eval()\n",
    "            mean_val_loss = 0\n",
    "            val_batches_n = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n",
    "                    if batch_i > max_batches_per_epoch_val:\n",
    "                        break\n",
    "\n",
    "                    batch_x = copy_data_to_device(batch_x, device)\n",
    "                    batch_y = copy_data_to_device(batch_y, device)\n",
    "\n",
    "                    pred = model(batch_x)\n",
    "                    loss = criterion(pred, batch_y)\n",
    "\n",
    "                    mean_val_loss += float(loss)\n",
    "                    val_batches_n += 1\n",
    "\n",
    "            mean_val_loss /= val_batches_n\n",
    "            print('Среднее значение функции потерь на валидации', mean_val_loss)\n",
    "\n",
    "            if mean_val_loss < best_val_loss:\n",
    "                best_epoch_i = epoch_i\n",
    "                best_val_loss = mean_val_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "                print('Новая лучшая модель!')\n",
    "            elif epoch_i - best_epoch_i > early_stopping_patience:\n",
    "                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n",
    "                    early_stopping_patience))\n",
    "                break\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step(mean_val_loss)\n",
    "\n",
    "            print()\n",
    "        except KeyboardInterrupt:\n",
    "            print('Досрочно остановлено пользователем')\n",
    "            break\n",
    "        except Exception as ex:\n",
    "            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n",
    "            # break\n",
    "\n",
    "    return best_val_loss, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ddd46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# заимствованный код из библиотеки dlnlputils\n",
    "def predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n",
    "    \"\"\"\n",
    "    :param model: torch.nn.Module - обученная модель\n",
    "    :param dataset: torch.utils.data.Dataset - данные для применения модели\n",
    "    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n",
    "    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n",
    "    :return: numpy.array размерности len(dataset) x *\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    results_by_batch = []\n",
    "\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        import tqdm\n",
    "        for batch_x, batch_y in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n",
    "            try:\n",
    "                batch_x = copy_data_to_device(batch_x, device)\n",
    "                if return_labels:\n",
    "                    labels.append(batch_y.numpy())\n",
    "                batch_pred = model(batch_x)\n",
    "                results_by_batch.append(batch_pred.detach().cpu().numpy())\n",
    "            except:\n",
    "                print('Ошибка')\n",
    "\n",
    "    if return_labels:\n",
    "        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n",
    "    else:\n",
    "        return np.concatenate(results_by_batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2d438af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, dataloader, lr=0.01, optimizer=None, loss_fn = torch.nn.NLLLoss(), epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    net.train() \n",
    "    total_loss, acc, count, i = 0, 0, 0, 0\n",
    "    for idx, (labels, features) in enumerate(dataloader):\n",
    "        print(idx)\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        acc += (predicted == labels).sum()\n",
    "        count += len(labels)\n",
    "        i += 1\n",
    "        if i % report_freq == 0:\n",
    "            print(f\"{count}: acc={acc.item() / count}\")\n",
    "        if epoch_size and count > epoch_size:\n",
    "            break\n",
    "    return total_loss.item() / count, acc.item() / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "642a9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bowify(b):\n",
    "    return (\n",
    "            torch.LongTensor([t[0]-1 for t in b]),\n",
    "            torch.stack([to_bow(t[1]) for t in b])\n",
    "    )\n",
    "\n",
    "train_dataset = SparseFeaturesDataset(train_X, train_y)\n",
    "test_dataset = SparseFeaturesDataset(test_X, test_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True,num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96808a8",
   "metadata": {},
   "source": [
    "# Model 1 (Linear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4766d4c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(UNIQUE_TOKENS_N, UNIQUE_LABELS_N)\n\u001b[1;32m      3\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m optim: \\\n\u001b[1;32m      4\u001b[0m     torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optim, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m best_val_loss, best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_eval_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mepoch_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43ml2_reg_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mlr_scheduler_ctor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 49\u001b[0m, in \u001b[0;36mtrain_eval_loop\u001b[0;34m(model, train_dataset, val_dataset, criterion, lr, epoch_n, batch_size, device, early_stopping_patience, l2_reg_alpha, max_batches_per_epoch_train, max_batches_per_epoch_val, data_loader_ctor, optimizer_ctor, lr_scheduler_ctor, shuffle_train, dataloader_workers_n)\u001b[0m\n\u001b[1;32m     47\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     48\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_ctor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39ml2_reg_alpha)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(UNIQUE_TOKENS_N, UNIQUE_LABELS_N)\n",
    "\n",
    "scheduler = lambda optim: \\\n",
    "    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "best_val_loss, best_model = train_eval_loop(model=model,\n",
    "                                            train_dataset=train_dataset,\n",
    "                                            val_dataset=test_dataset,\n",
    "                                            criterion=F.cross_entropy,\n",
    "                                            lr=1e-1,\n",
    "                                            epoch_n=50,\n",
    "                                            batch_size=32,\n",
    "                                            l2_reg_alpha=0,\n",
    "                                            lr_scheduler_ctor=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3bae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2ebc24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), 'pt_linear_model_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7e949",
   "metadata": {},
   "source": [
    "# Model 2 (Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f706df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.nn import CNNEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1aec53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionNeuralNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 num_filters, \n",
    "                 ngram_filter_sizes=(2, 3, 4), \n",
    "                 activation=nn.ReLU(),\n",
    "                 output_dim=None):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_filters = num_filters\n",
    "        self.ngram_filter_sizes = ngram_filter_sizes\n",
    "        self.activation = activation\n",
    "        self.output_dim = output_dim\n",
    "        self.conv_layers = [\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=ngram_size)\n",
    "            for ngram_size in ngram_filter_sizes\n",
    "        ]\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            self.add_module('conv_layer_%d' % i, conv_layer)\n",
    "        maxpool_output_dim = num_filters * len(ngram_filter_sizes)\n",
    "        if output_dim:\n",
    "            self.projection_layer = nn.Linear(maxpool_output_dim, output_dim)\n",
    "        else:\n",
    "            self.projection_layer = None\n",
    "            self.output_dim = maxpool_output_dim\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        '''\n",
    "        tokens: FloatTensor[batch_size, num_tokens, input_dim]\n",
    "        '''\n",
    "        tokens = torch.transpose(tokens, 1, 2)\n",
    "        \n",
    "        filter_outputs = []\n",
    "        for i in range(len(self.conv_layers)):\n",
    "            conv_layer = getattr(self, 'conv_layer_%d' % i)\n",
    "            filter_outputs.append(self.activation(conv_layer(tokens)).max(dim=2)[0])\n",
    "        maxpool_output = torch.cat(filter_outputs, dim=1) if len(filter_outputs) > 1 else filter_outputs[0]\n",
    "        \n",
    "        if self.projection_layer:\n",
    "            result = self.projection_layer(maxpool_output)\n",
    "        else:\n",
    "            result = maxpool_output\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80b6eb0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     50\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 51\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     53\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m, in \u001b[0;36mTextCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, seq_len, embed_dim]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, embed_dim, seq_len]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     x_conv \u001b[38;5;241m=\u001b[39m [nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(conv(x)) \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers]  \u001b[38;5;66;03m# [(batch_size, num_filters, conv_seq_len), ...]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_filters, filter_sizes, num_classes):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=fs) for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)  # [batch_size, seq_len, embed_dim]\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, embed_dim, seq_len]\n",
    "        x_conv = [nn.functional.relu(conv(x)) for conv in self.conv_layers]  # [(batch_size, num_filters, conv_seq_len), ...]\n",
    "        x_pool = [nn.functional.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in x_conv]  # [(batch_size, num_filters), ...]\n",
    "        x_fc = torch.cat(x_pool, dim=1)  # [batch_size, num_filters * len(filter_sizes)]\n",
    "        x_out = self.fc(x_fc)  # [batch_size, num_classes]\n",
    "        return x_out\n",
    "\n",
    "# Define hyperparameters\n",
    "vocab_size = UNIQUE_TOKENS_N\n",
    "embed_dim = 100\n",
    "num_filters = 128\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_classes = UNIQUE_LABELS_N\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# Define dataset and dataloader\n",
    "train_dataset = train_dataset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = test_dataset\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = TextCNN(vocab_size, embed_dim, num_filters, filter_sizes, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    for inputs, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_acc += torch.sum(preds == labels.data)\n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    train_acc = train_acc / len(train_dataset)\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_acc += torch.sum(preds == labels.data)\n",
    "        val_loss = val_loss / len(val_dataset)\n",
    "        val_acc = val_acc / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02abc84c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ConvolutionNeuralNetwork.__init__() got an unexpected keyword argument 'features_num'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m conv_model \u001b[38;5;241m=\u001b[39m \u001b[43mConvolutionNeuralNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUNIQUE_TOKENS_N\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m optim: \\\n\u001b[1;32m      4\u001b[0m     torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optim, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m best_val_loss, best_model \u001b[38;5;241m=\u001b[39m train_eval_loop(\n\u001b[1;32m      7\u001b[0m                                            conv_model,\n\u001b[1;32m      8\u001b[0m                                            train_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m                                            lr_scheduler_ctor\u001b[38;5;241m=\u001b[39mscheduler\n\u001b[1;32m     16\u001b[0m                                            )\n",
      "\u001b[0;31mTypeError\u001b[0m: ConvolutionNeuralNetwork.__init__() got an unexpected keyword argument 'features_num'"
     ]
    }
   ],
   "source": [
    "conv_model = ConvolutionNeuralNetwork(features_num=UNIQUE_TOKENS_N, layers_n=5, dropout=0.1)\n",
    "\n",
    "scheduler = lambda optim: \\\n",
    "    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "best_val_loss, best_model = train_eval_loop(\n",
    "                                           conv_model,\n",
    "                                           train_dataset,\n",
    "                                           test_dataset,\n",
    "                                           F.cross_entropy,\n",
    "                                           lr=1e-1,\n",
    "                                           epoch_n=50,\n",
    "                                           batch_size=32,\n",
    "                                           l2_reg_alpha=0,\n",
    "                                           lr_scheduler_ctor=scheduler\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134cb3d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0\n",
      "Эпоха: 10001 итераций, 61.34 сек\n",
      "Среднее значение функции потерь на обучении 1.094627337620957\n",
      "Среднее значение функции потерь на валидации 0.9286486540998254\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 1\n",
      "Эпоха: 10001 итераций, 61.21 сек\n",
      "Среднее значение функции потерь на обучении 1.0057485575479765\n",
      "Среднее значение функции потерь на валидации 0.8762897318536109\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 2\n",
      "Эпоха: 10001 итераций, 60.45 сек\n",
      "Среднее значение функции потерь на обучении 1.00187811619305\n",
      "Среднее значение функции потерь на валидации 0.8735460841155552\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 3\n",
      "Эпоха: 10001 итераций, 60.67 сек\n",
      "Среднее значение функции потерь на обучении 1.0027435652411778\n",
      "Среднее значение функции потерь на валидации 0.8817427166989752\n",
      "\n",
      "Эпоха 4\n",
      "Эпоха: 10001 итераций, 61.92 сек\n",
      "Среднее значение функции потерь на обучении 1.006321900922672\n",
      "Среднее значение функции потерь на валидации 0.897151441096545\n",
      "\n",
      "Эпоха 5\n",
      "Эпоха: 10001 итераций, 66.91 сек\n",
      "Среднее значение функции потерь на обучении 1.0054317902069236\n",
      "Среднее значение функции потерь на валидации 0.8787709732989332\n",
      "\n",
      "Эпоха 6\n",
      "Эпоха: 10001 итераций, 64.96 сек\n",
      "Среднее значение функции потерь на обучении 1.0108466427238711\n",
      "Среднее значение функции потерь на валидации 0.9067816598551137\n",
      "\n",
      "Эпоха 7\n",
      "Эпоха: 10001 итераций, 67.87 сек\n",
      "Среднее значение функции потерь на обучении 1.0040860201624462\n",
      "Среднее значение функции потерь на валидации 0.9020846972575078\n",
      "\n",
      "Эпоха 8\n",
      "Эпоха: 10001 итераций, 68.63 сек\n",
      "Среднее значение функции потерь на обучении 1.0110440620743337\n",
      "Среднее значение функции потерь на валидации 0.9133280737476273\n",
      "Epoch 00009: reducing learning rate of group 0 to 5.0000e-02.\n",
      "\n",
      "Эпоха 9\n",
      "Эпоха: 10001 итераций, 66.02 сек\n",
      "Среднее значение функции потерь на обучении 0.9380678598397542\n",
      "Среднее значение функции потерь на валидации 0.8562469691425175\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 10\n",
      "Эпоха: 10001 итераций, 65.63 сек\n",
      "Среднее значение функции потерь на обучении 0.9325902031762708\n",
      "Среднее значение функции потерь на валидации 0.841458537927517\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 11\n",
      "Эпоха: 10001 итераций, 67.50 сек\n",
      "Среднее значение функции потерь на обучении 0.93300567861319\n",
      "Среднее значение функции потерь на валидации 0.8432399389686641\n",
      "\n",
      "Эпоха 12\n",
      "Эпоха: 10001 итераций, 70.75 сек\n",
      "Среднее значение функции потерь на обучении 0.9314681033547694\n",
      "Среднее значение функции потерь на валидации 0.842695144119558\n",
      "\n",
      "Эпоха 13\n",
      "Эпоха: 10001 итераций, 69.43 сек\n",
      "Среднее значение функции потерь на обучении 0.9346481870650148\n",
      "Среднее значение функции потерь на валидации 0.8354671714784621\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 14\n",
      "Эпоха: 10001 итераций, 64.07 сек\n",
      "Среднее значение функции потерь на обучении 0.9326687741787383\n",
      "Среднее значение функции потерь на валидации 0.854100150572551\n",
      "\n",
      "Эпоха 15\n",
      "Эпоха: 10001 итераций, 64.41 сек\n",
      "Среднее значение функции потерь на обучении 0.9304357473343184\n",
      "Среднее значение функции потерь на валидации 0.8347584036859004\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 16\n",
      "Эпоха: 10001 итераций, 64.16 сек\n",
      "Среднее значение функции потерь на обучении 0.93345653499118\n",
      "Среднее значение функции потерь на валидации 0.8462107392041\n",
      "\n",
      "Эпоха 17\n",
      "Эпоха: 10001 итераций, 64.23 сек\n",
      "Среднее значение функции потерь на обучении 0.9332344575102789\n",
      "Среднее значение функции потерь на валидации 0.8496834332947726\n",
      "\n",
      "Эпоха 18\n",
      "Эпоха: 10001 итераций, 64.84 сек\n",
      "Среднее значение функции потерь на обучении 0.9320125560166418\n",
      "Среднее значение функции потерь на валидации 0.8495020532048309\n",
      "\n",
      "Эпоха 19\n",
      "Эпоха: 10001 итераций, 64.99 сек\n",
      "Среднее значение функции потерь на обучении 0.9287040078983273\n",
      "Среднее значение функции потерь на валидации 0.8551133134803334\n",
      "\n",
      "Эпоха 20\n",
      "Эпоха: 10001 итераций, 65.18 сек\n",
      "Среднее значение функции потерь на обучении 0.9315544300294616\n",
      "Среднее значение функции потерь на валидации 0.8469447271152214\n",
      "\n",
      "Эпоха 21\n",
      "Эпоха: 10001 итераций, 66.09 сек\n",
      "Среднее значение функции потерь на обучении 0.9321181782572618\n",
      "Среднее значение функции потерь на валидации 0.8380058359254252\n",
      "Epoch 00022: reducing learning rate of group 0 to 2.5000e-02.\n",
      "\n",
      "Эпоха 22\n",
      "Эпоха: 10001 итераций, 67.02 сек\n",
      "Среднее значение функции потерь на обучении 0.9013116258690922\n",
      "Среднее значение функции потерь на валидации 0.813630112163075\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 23\n",
      "Эпоха: 10001 итераций, 64.32 сек\n",
      "Среднее значение функции потерь на обучении 0.8960577272770465\n",
      "Среднее значение функции потерь на валидации 0.8157001211867109\n",
      "\n",
      "Эпоха 24\n",
      "Эпоха: 10001 итераций, 64.20 сек\n",
      "Среднее значение функции потерь на обучении 0.9038356743673243\n",
      "Среднее значение функции потерь на валидации 0.8198487646393962\n",
      "\n",
      "Эпоха 25\n",
      "Эпоха: 10001 итераций, 64.45 сек\n",
      "Среднее значение функции потерь на обучении 0.8989613378760744\n",
      "Среднее значение функции потерь на валидации 0.809566626598785\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 26\n",
      "Эпоха: 10001 итераций, 69.23 сек\n",
      "Среднее значение функции потерь на обучении 0.9039988128790414\n",
      "Среднее значение функции потерь на валидации 0.8080473878940979\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 27\n"
     ]
    }
   ],
   "source": [
    "seq_model = nn.Sequential(\n",
    "           nn.Dropout(0.05),\n",
    "           nn.Linear(UNIQUE_TOKENS_N, UNIQUE_LABELS_N)\n",
    ")\n",
    "\n",
    "\n",
    "scheduler = lambda optim: \\\n",
    "    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "best_val_loss, best_model = train_eval_loop(model=seq_model,\n",
    "                                            train_dataset=train_dataset,\n",
    "                                            val_dataset=test_dataset,\n",
    "                                            criterion=F.cross_entropy,\n",
    "                                            lr=1e-1,\n",
    "                                            epoch_n=50,\n",
    "                                            batch_size=64,\n",
    "                                            l2_reg_alpha=0,\n",
    "                                            lr_scheduler_ctor=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "355816b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(seq_model.state_dict(), 'linear_dropout_0.1_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d6a0fcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(UNIQUE_TOKENS_N, UNIQUE_LABELS_N)\n\u001b[0;32m----> 2\u001b[0m loaded_model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt_linear_model_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m loaded_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1112\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39mUntypedStorage)\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1116\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1117\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1121\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/serialization.py:187\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_utils.py:81\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_type(indices, values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = nn.Linear(UNIQUE_TOKENS_N, UNIQUE_LABELS_N)\n",
    "loaded_model.load_state_dict(torch.load('pt_linear_model_1'))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d73cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_with_model(best_model, test_dataset)\n",
    "loss = F.cross_entropy(torch.from_numpy(pred), \n",
    "                       torch.from_numpy(test_y).long())\n",
    "\n",
    "print('Среднее значение функции потерь на обучении', float(loss))\n",
    "print('Доля верных ответов', accuracy_score(test_y, pred.argmax(-1)))\n",
    "print('Precision', precision_score(test_y, pred.argmax(-1), average='macro'))\n",
    "print('Recall', recall_score(test_y, pred.argmax(-1), average='macro'))\n",
    "print('F1', f1_score(test_y, pred.argmax(-1), average='macro'))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
